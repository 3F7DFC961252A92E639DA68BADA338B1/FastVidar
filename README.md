# FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention

<p align="center">
  <b><a href="https://3F7DFC.github.io/FastVidar/">Project Page</a></b> Â·
  <a href="#-news">News</a> Â·
  <a href="#-quick-start">Quick Start</a> Â·
  <a href="#-models--checkpoints">Models</a> Â·
  <a href="#-citation">Citation</a>
</p>

<p align="center">
  <img src="assets/figures/teaser.png" alt="FastViDAR teaser" width="75%"/>
</p>


## ğŸ” TL;DR
FastViDAR takes four fisheye camera inputs and produces a full 360Â° depth map alongside per-camera depth, fused ERP depth, and confidence maps â€” in real time. We propose Alternative Hierarchical Attention (AHA), which alternates window-level local self-attention with per-frame and global self-attention over compact summary tokens, enabling strong cross-view reasoning at low overhead. We further fuse multi-view depths on a shared equirectangular (ERP) grid for seamless omnidirectional depth. FastViDAR reaches competitive accuracy with up to 20 FPS on NVIDIA Orin NX (TensorRT FP16) and 3.3Ã— speedup over VGGT at 640Ã—320 with 4 views.


## âœ¨ Highlights
- âš¡ï¸ Alternative Hierarchical Attention (AHA): alternating local window attention with frame/global attention over summary tokens; ~16Ã— theoretical reduction vs full attention at typical settings.
- ğŸŒ ERP-centric pipeline: fisheye views are unified on an equirectangular grid, removing lens-specific intrinsics from learning.
- ğŸ” ERP fusion: distance-aware splatting and masked averaging on ERP yield consistent, seamless 360Â° depth.
- ğŸ›  Flexible camera rigs: supports arbitrary numbers of cameras and ultra-wide FOV (>180Â°).
- ğŸš€ Real-time and deployable: up to 20 FPS on NVIDIA Orin NX (embedded, TensorRT FP16) and high throughput on desktop GPUs.


## ğŸ— News
- Codebase, configs, and pretrained checkpoints will be released progressively. Watch the repo to get updates.
- Demo video and arXiv preprint are coming soon.


## ğŸ”— Links
- Project Page: https://3F7DFC.github.io/FastVidar/
- Paper (ICRA): to appear (arXiv TBA)
- Demo Video: TBA


## ğŸ§­ Table of Contents
- [TL;DR](#-tldr)
- [Highlights](#-highlights)
- [News](#-news)
- [Links](#-links)
- [Installation](#-installation)
- [Datasets](#-datasets)
- [Models & Checkpoints](#-models--checkpoints)
- [Quick Start](#-quick-start)
- [Training](#-training)
- [Evaluation](#-evaluation)
- [Inference Demo](#-inference-demo)
- [Benchmarks](#-benchmarks)
- [Visualization](#-visualization)
- [Citation](#-citation)
- [License](#-license)
- [Acknowledgements](#-acknowledgements)
- [Contact](#-contact)


## ğŸ”§ Installation (TBD)
We recommend Python â‰¥ 3.10 and PyTorch â‰¥ 2.7.0. Adjust CUDA wheels to your environment.

```bash
git clone https://github.com/3F7DFC/FastVidar.git
cd FastVidar

# Optional: create a virtual environment
python3 -m venv .venv && source .venv/bin/activate

# Install dependencies (replace with the official requirements when released)
pip install -U pip
pip install -r requirements.txt  # if provided

# Minimal example (adjust CUDA index URL to your setup)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install numpy opencv-python matplotlib einops tqdm pyyaml
```


## ğŸ“¦ Datasets (TBD)
We provide ERP-aware training and evaluation on HM3D and Stanford 2D-3D-S, plus custom data. Organize your data as follows:

```
datasets/
  <dataset_name>/
    images/            # input images (ERP/fisheye/surround-view)
    depths/            # ground-truth depth, if available
    splits/            # train/val/test split files (optional)
```

For custom datasets, configure paths and camera models (e.g., ERP projection, fisheye intrinsics) in `configs/<your_dataset>.yaml`.

Dataset notes (from the paper):
- HM3D (train/ablate): rendered 4-camera rigs with randomized relative poses and FOV in [160Â°, 360Â°]; 800 train / 200 test scenes.
- 2D-3D-S (zero-shot): a fixed 4-fisheye ring (FOV 220Â°) with 90Â° separation; evaluated without fine-tuning.


## ğŸ§  Models & Checkpoints (TBD)
Pretrained weights will be released here. Place them under:

```
checkpoints/
  fastvidar_<variant>.pth
```


## ğŸš€ Quick Start (TBD)

```bash
# Single-GPU training example
python tools/train.py \
  --config configs/fastvidar_base.yaml \
  --data-root datasets/<dataset_name> \
  --work-dir work_dirs/fastvidar_base

# Evaluation
python tools/eval.py \
  --config configs/fastvidar_base.yaml \
  --data-root datasets/<dataset_name> \
  --checkpoint checkpoints/fastvidar_base.pth

# Inference demo
python tools/demo.py \
  --input assets/example.jpg \
  --checkpoint checkpoints/fastvidar_base.pth \
  --output outputs/example_depth.png
```


## ğŸ‹ï¸ Training (TBD)
Multi-GPU (distributed) training example; adjust to your cluster setup:

```bash
torchrun --nproc_per_node=8 tools/train.py \
  --config configs/fastvidar_base.yaml \
  --data-root datasets/<dataset_name> \
  --work-dir work_dirs/fastvidar_base_dist
```

Key configuration knobs (typical in `configs/*.yaml`):
- Model variant, AHA block counts, window size (e.g., 7Ã—7), and summary tokens.
- Input resolution and ERP specifics; number of cameras/views S (e.g., S=4).
- Loss: masked ERP-area weighted depth loss with multi-scale gradient term.
- Optimizer/schedule: AdamW + OneCycleLR (example); AMP/mixed precision.


## ğŸ“ Evaluation (TBD)
Supports common depth metrics on ERP: AbsRel, RMSE, Log10, and Î´<1.25. Example:

```bash
python tools/eval.py \
  --config configs/fastvidar_base.yaml \
  --data-root datasets/<dataset_name> \
  --checkpoint checkpoints/fastvidar_base.pth
```

By default, results are saved under `work_dirs/<exp>/eval/`.


## ğŸ¬ Inference Demo (TBD)
Single image or video stream inference:

```bash
python tools/demo.py \
  --input assets/example.jpg \
  --checkpoint checkpoints/fastvidar_base.pth \
  --output outputs/example_depth.png
```

For video/webcam streams, see script flags in `tools/demo.py` (e.g., `--video`, `--webcam`, `--fps`).


## ğŸ“Š Benchmarks (TBD)
Zero-shot comparison on 2D-3D-S (selected metrics):

| Method        | AbsRel â†“ | RMSE â†“ | Log10 â†“ | Î´<1.25 â†‘ | Time (ms) |
|---------------|----------|--------|---------|----------|-----------|
| VGGT          | 0.557    | 1.934  | 0.396   | 0.043    | 120       |
| OmniStereo    | 0.619    | 1.450  | 0.154   | 0.554    | 66        |
| LightStereo-M | 0.125    | 0.667  | 0.050   | 0.851    | 33        |
| FastViDAR     | 0.119    | 0.433  | 0.046   | 0.929    | 36        |

Notes:
- FastViDAR uses 4 ERP views (FOV 220Â° each). LightStereo uses ERP splitting to stereo; VGGT uses 8 pinhole projections (100Â° FOV) and is projected back to ERP for evaluation.
- On Orin NX with TensorRT FP16, FastViDAR reaches up to 20 FPS.

Assets placeholders:

```
assets/figures/
  qualitative_samples.png
  speed_accuracy_tradeoff.png
```


## ğŸ–¼ Visualization (TBD)
- Qualitative comparisons across ERP/fisheye/surround-view.
- Error maps and fused point clouds on ERP.
- Latency and throughput analyses across hardware.

We will provide ready-to-use notebooks and scripts under `tools/`.


## ğŸ“š Citation (TBD)
If you find FastViDAR useful, please cite:

```bibtex
@inproceedings{fastvidar_icra2026,
  title     = {FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative Hierarchical Attention},
  author    = {Author, A. and Author, B. and Author, C.},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2026},
  note      = {Project page: https://3F7DFC.github.io/FastVidar/}
}
```


## ğŸ“„ License (TBD)
This repository will be released under the MIT License (subject to change until `LICENSE` is added). Please check the repository for updates.


## ğŸ™ Acknowledgements (TBD)
We thank the authors and maintainers of related open-source projects and datasets. Our implementation is inspired by community best practices; we will add detailed references upon release.


## âœ‰ï¸ Contact (TBD)
For questions, suggestions, or collaborations, please open an issue or contact the authors (emails TBA).

 
